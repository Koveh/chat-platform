---
title: "Exam Prep — Statistical and Machine Learning (WS 2025/26)"
author: "Daniil Koveh"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

# Как читать
Пишу подробно, но без лишней болтовни. Для каждого вопроса: определения, формулы, интуиция, житейский пример, типичные ошибки, контрольные вопросы, шпаргалка. Стиль — как у Ильяхова: короткие фразы, ясные выводы, конкретные списки. Цель — чтобы за один проход было ясно, где логика и почему формулы такие.

# Question 1. Базовые понятия и bias–variance
## 1.1 Supervised vs Unsupervised
- **Supervised (обучение с учителем).** Есть пары $(x, y)$. Строим $f(x)$, чтобы предсказывать $y$. Пример: прогноз цены квартиры по площади, этажу и локации. Логика: минимизируем потерю между предсказанием и реальностью.
- **Unsupervised (обучение без учителя).** Есть только $x$. Ищем структуру: кластеры, аномалии, снижение размерности. Пример: группировка клиентов по поведению без меток «VIP/не VIP». Логика: оптимизируем критерий внутренней согласованности (например, k-means — минимизация суммы квадратов расстояний до центроидов).
- **Полу- и самообучение** — гибриды: мало меток или метки по самим данным (autoencoder).

## 1.2 Bias–Variance trade-off
- **Смещение (bias).** Насколько систематически модель промахивается относительно истинной функции. Простая модель — высокий bias.
- **Разброс (variance).** Насколько чувствительна модель к выборке. Сложная модель — высокий variance.
- **Суммарная ошибка** = шум данных + bias^2 + variance. Подбор модели — балансировать bias и variance под размер и шум выборки.
- **Реальная жизнь.** Полином степени 1 плохо аппроксимирует синус (высокий bias, низкий variance). Полином степени 20 на 30 точках — низкий bias на train, высокий variance на test. Середина (степень 5–7) — компромисс.

## 1.3 Вывод ожидаемой квадратичной ошибки в точке $x_0$
Пусть $Y = f(X) + \varepsilon$, $\\mathbb{E}[\\varepsilon]=0$, $\\mathrm{Var}(\\varepsilon)=\\sigma^2$. Процедура обучает $\\hat f$ на выборке $T$. Рассматриваем $\\hat f(x_0)$.
\\[
\\mathbb{E}_{T,\\varepsilon}[(Y_0 - \\hat f(x_0))^2] = \\underbrace{\\sigma^2}_{\\text{неустранимый шум}} + \\underbrace{\\big(\\mathbb{E}_T[\\hat f(x_0)] - f(x_0)\\big)^2}_{\\text{bias}^2} + \\underbrace{\\mathrm{Var}_T[\\hat f(x_0)]}_{\\text{variance}}.
\\]
Шаги:
1. Разложите $(Y_0 - \\hat f)^2 = (f + \\varepsilon - \\hat f)^2$.
2. Вставьте $\\mathbb{E}_\\varepsilon[\\varepsilon]=0$ и $\\mathbb{E}_\\varepsilon[\\varepsilon^2]=\\sigma^2$.
3. Добавьте и вычтите $\\mathbb{E}_T[\\hat f]$ внутри квадрата, чтобы выделить bias и variance.

## 1.4 Линейная модель vs k-NN через bias–variance
- **Линейка.** Предполагает линейность → высокий bias, низкий variance. Лучше на малых выборках, устойчивее к шуму и лишним признакам, быстро обучается.
- **k-NN.** Почти нет предположений → низкий bias при малом k, но высокий variance. Требует много данных, страдает от размерности. Увеличение k повышает bias, снижает variance.
- **Пример.** Прогноз зарплаты по стажу и образованию: линейка может быть норм. k-NN лучше ухватит нелинейность (рост до 5 лет, плато), но потребует много наблюдений.

## 1.5 Шпаргалка
- Bias лечим усложнением модели; variance — регуляризацией, усреднением (bagging), снижением сложности.
- k-NN и деревья — высокое variance, низкое bias; линейка и наивный Байес — наоборот.
- Не забывайте про шум: ниже $\\sigma^2$ не прыгнуть.

# Question 2. Функции потерь и проклятие размерности
## 2.1 Что такое функция потерь
- Отображает ошибку прогноза в штраф. Задаёт цель оптимизации.
- Свойства: минимальное значение при идеальном предсказании; часто выпуклая (облегчает оптимизацию), иногда гладкая (градиенты).
- Чувствительность к выбросам: MSE штрафует квадратично; MAE — линейно.

## 2.2 Примеры
- **Регрессия:** MSE $(y-\\hat y)^2$; MAE $|y-\\hat y|$; Huber — гибрид (квадрат для малых ошибок, линейный хвост).
- **Классификация:** логистическая (deviance) $-y\\log p - (1-y)\\log(1-p)$ для $y\\in\\{0,1\\}$; экспоненциальная $\\exp(-y f)$ для $y\\in\\{-1,1\\}$; hinge $\\max(0, 1 - y f)$ для SVM.
- **Калибровка:** Proper scoring rules (Brier, log-loss) поощряют честные вероятности.

## 2.3 Проклятие размерности
- Объём гиперкуба растёт как $L^d$. При фиксированном $n$ точки разрежены, расстояния между точками сближаются, локальные методы теряют смысл.
- Оценка плотности и k-NN требуют экспоненциального роста $n$ для сохранения плотности.

## 2.4 Пример и последствия
- Хотим k-NN в 100D. Чтобы шарик радиуса $r$ покрывал хотя бы 1% объёма гиперкуба, $r$ должен быть ~0.95. «Ближайший» сосед далеко → высокая ошибка, variance взлетает.
- Вывод: снижайте размерность (PCA, автоэнкодер), делайте отбор признаков, используйте модели, устойчивые к разреженности (линейка с регуляризацией, деревья с mtry).

## 2.5 Шпаргалка
- Подбирайте потерю под задачу: MSE — для Гаусса; логистическая — для вероятностей; hinge — для максимизации отступа.
- В высоких измерениях избегайте чистых локальных методов без отбора/редукции.

# Question 3. Отбор признаков и shrinkage
## 3.1 Стратегии отбора подмножеств
- Best subset: полный перебор, оптимален по критерию, но экспоненциально дорог.
- Forward selection: начинаем с пустоты, добавляем лучший признак по критерию (AIC/BIC/CV).
- Backward elimination: старт с полного набора, удаляем худший; требует $n>p$.
- Stepwise: смесь forward/backward.
- Регуляризация как мягкий отбор (Lasso/Elastic Net).

## 3.2 Модификация критерия при shrinkage
OLS: $\\min \\|y - X\\beta\\|_2^2$. Добавляем штраф:
- Ridge: $+ \\lambda \\|\\beta\\|_2^2$ (L2).
- Lasso: $+ \\lambda \\|\\beta\\|_1$ (L1).
- Elastic Net: $+ \\lambda_1 \\|\\beta\\|_1 + \\lambda_2 \\|\\beta\\|_2^2$.
Штраф сжимает коэффициенты, контролирует variance, иногда зануляет (L1).

## 3.3 Ridge vs Lasso
- Ridge: сжимает все, не зануляет. Хорош при мультиколлинеарности, много слабых предикторов. Решается аналитически через SVD или гребневую формулу. df = trace$\\big(X (X^T X + \\lambda I)^{-1} X^T\\big)$.
- Lasso: даёт разреженность, отбор признаков. Несглаженная L1 → координатный спуск, LARS. Может непредсказуемо выбирать один из коррелированных признаков.
- Elastic Net: компромисс, группирует коррелированные признаки, но оставляет разреженность.
- Подбор $\\lambda$: CV, информационные критерии (AICc для Lasso), stability selection.

## 3.4 Практические вопросы
- Стандартизация обязательна: иначе штрафы не сопоставимы по осям.
- Гиперпараметры: сетка $\\lambda$ (и $\\alpha$ для EN). CV K-fold, лучше повторный CV для устойчивости.
- Эффективные степени свободы: у Lasso грубо равно числу ненулевых; у Ridge есть закрытая формула.
- Проверяйте утечку информации: стандартизацию и CV делайте в пайплайне, не на всей выборке.

## 3.5 Житейский пример
Прогноз выручки магазина по 200 признакам (погода, акции, конкуренты). Ridge удержит все признаки и сгладит шум. Lasso удалит неинформативные, оставит несколько важных (например, скидки и сезонность).

# Question 4. GLM и регуляризация
## 4.1 Определение GLM
- Составные части: (1) Распределение из экспоненциального семейства (Норм, Бернулли, Пуассон, Гамма). (2) Линейный предиктор $\\eta = X\\beta$. (3) Связь $g(\\mu)=\\eta$, где $\\mu=\\mathbb{E}[Y|X]$.
- Примеры: логистическая регрессия (Бернулли + логит), Пуассон-регрессия (лог-ссылка), Gamma (inverse link).

## 4.2 Оценка и выводы
- Оценка: MLE через IRLS (итеративные взвешенные МНК) ≈ Ньютона-Рафсона на лог-правдоподобии.
- Инференс: ковариация оценивается через обратную Фишера. Wald интервалы и тесты; score/LR тесты для гипотез.
- Диагностика: девианс, псевдо-$R^2$, проверка линейности логитов, остатки Пирсона/девиационные, влияния (Cook).

## 4.3 Регуляризованные GLM
- Добавляем штраф к лог-правдоподобию: $\\ell(\\beta) - \\lambda \\|\\beta\\|_1$ (Lasso) или $-\\lambda\\|\\beta\\|_2^2$ (Ridge).
- Алгоритмы: координатный спуск (glmnet), мягкий порог для L1.
- Подбор $\\lambda$ — CV; модельные выборки — по минимальному CV-девиансу или 1-SE правилу.

## 4.4 (Квази-)полная сепарация
- В бинарном логите, если классы разделимы линейно, MLE уходит к бесконечным коэффициентам. Признаки: огромные стандартные ошибки, несходимость.
- Лечение: Ridge/Firth (penalized likelihood), уменьшить сложность, собрать больше данных, добавить шум, использовать Байесовский слабый prior.

## 4.5 Пример
Предсказание расклика по числу показов: Пуассон (log link). Если добавили бинарный признак, идеально отделяющий удачные кампании, логит может взлететь — добавьте Ridge.

# Question 5. Оценка и выбор моделей
## 5.1 Assessment vs Selection
- Assessment: оценить качество одной модели (например, финальной) — нужны честные метрики (test/CV/OOB).
- Selection: выбрать лучшую из набора — сравниваем по одной метрике, учитываем variance оценки.

## 5.2 Training, test, expected test
- Training error: оптимистичен, смещён вниз, не подходит для выбора между разными сложностями (overfit выигрывает).
- Test error: честен, но зависит от конкретного разбиения.
- Expected test error: матожидание test error по выборкам/разбиениям. Оценим через CV/bootstrapping или большой hold-out.

## 5.3 In-sample error и оптимизм
- In-sample: ожидаемая ошибка на том же распределении при обучении на всех данных.
- Optimism: насколько train error занижает in-sample. Для квадратичной ошибки с линейной моделью: Optimism $\\approx 2\\sigma^2 \\frac{df}{n}$.
- Интуиция: чем выше df (сложность), тем сильнее оптимизм.

## 5.4 Методы оценки in-sample через train error
- $C_p$: $\\text{train MSE} + 2\\hat\\sigma^2 df / n$.
- AIC: $-2\\ell + 2 df$ (для лог-правдоподобия).
- BIC: сильнее штраф $\\log n$.
- Adjusted $R^2$: корректирует за df.
- Для классификации: аналог через deviance + штраф.
- Подходит для selection; для финальной оценки — всё равно test/CV.

## 5.5 Пример
Сравниваем полиномы степеней 1–10. Train MSE падает монотонно. $C_p$/AIC/BIC сначала падают, потом растут — выбираем минимум. Потом валидируем на отложке.

# Question 6. Кросс-валидация и бутстреп
## 6.1 Цели CV
- Оценить ожидаемую ошибку, не тратя тестовые данные.
- Можно использовать для выбора гиперпараметров и для финальной оценки (но уже без повторного тюнинга).

## 6.2 Схема K-fold
- Разбиваем на K фолдов. Для каждого k: обучаем на K-1, валидируем на k-й, сохраняем метрику. Усредняем.
- Выбор K: 5–10 — хороший баланс. Малое K → высокий bias, низкий variance, дёшево. Большое (LOOCV) → низкий bias, высокий variance, дорого.
- Стратификация в классификации важна, чтобы классы были представлены в каждом фолде.
- Повторный CV (repeated CV) снижает variance оценки.

## 6.3 Цели bootstrapping
- Оценить разброс, доверительные интервалы, стабильность коэффициентов, важность признаков.
- Для ошибки: использовать OOB примеры (не попавшие в bootstrap) как валид. тест.

## 6.4 Схема bootstrap для ошибки
- Семплируем с возвращением $n$ раз → бутстрап-выборка (~63.2% уникальных).
- Обучаем модель, считаем ошибку на OOB.
- Усредняем ошибки. .632/.632+ комбинирует train и OOB, чтобы компенсировать смещение.
- Минусы: зависимость сэмплов, смещение у нестабильных моделей, для сильных регуляризаторов CV надёжнее.

## 6.5 Пример
Оцениваем логит на малой выборке (200 наблюдений). CV 10-fold даёт устойчивую оценку AUC. Bootstrap .632 даёт чуть оптимистичнее. Для выбора числа признаков — CV; для доверительных интервалов коэффициентов — bootstrap.

# Question 7. Деревья решений
## 7.1 Цель
Строить правила вида «если-признак-∘-порог → предсказание», чтобы ловить нелинейности и взаимодействия, сохраняя интерпретируемость.

## 7.2 Плюсы и минусы
**Плюсы:**\n- Понятные правила, визуализация.\n- Работают с числовыми/категориальными признаками без стандартизации.\n- Естественно обрабатывают пропуски (суррогатные сплиты).\n\n**Минусы:**\n- Высокий variance: маленькое изменение данных → другое дерево.\n- Склонность к переобучению без ограничений глубины/прюнинга.\n- Жадность сплитов: может пропустить глобально лучший разрез.\n

## 7.3 Общий алгоритм
1. Выбираем признак и порог, максимизирующие снижение импьюрити (Gini/entropy для классификации, MSE для регрессии).\n2. Разбиваем данные, рекурсивно повторяем в дочерних узлах.\n3. Остановка: max_depth, min_samples_leaf, min_impurity_decrease, max_leaf_nodes.\n4. Обрезка (cost-complexity pruning): подбираем параметр $\\alpha$ на валидации, удаляем слабые ветви.\n\nВарианты: CART (Gini/MSE), C4.5 (information gain, handling категорий), CHAID (хи-квадрат), oblique trees (линейные сплиты).

## 7.4 Гиперпараметры
- max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes.\n- criterion (Gini/entropy/MSE).\n- min_impurity_decrease.\n- class_weight для несбалансированных.\nВыбор — через CV/OOB. Ошибки: слишком глубокое → overfit; слишком мелкое → underfit.

## 7.5 Пример
Скоринг кредитов: дерево учит правила «если доход < X и задолженность > Y → риск высокий». Глубину ограничиваем, чтобы не заучивать редкие сочетания.

# Question 8. Bagging
## 8.1 Идея и варианты
Усреднить много моделей, обученных на бутстрапах, чтобы снизить variance. Варианты: bagging деревьев (Classic), bagging k-NN, bagging линейной регрессии (редко, но можно для устойчивости).

## 8.2 Алгоритм
1. Для b=1..B: бутстрап выборка, обучить базовый learner.\n2. Прогноз: среднее (регрессия) или большинство голосов (классификация).\n3. Можно оценить OOB-ошибку без выделенного теста.\n

## 8.3 Почему работает (MSE)
\\[
\\mathrm{Var}(\\bar f) = \\rho \\sigma^2 + (1-\\rho) \\frac{\\sigma^2}{B},
\\]\nгде $\\rho$ — корреляция базовых ошибок, $\\sigma^2$ — их variance. Усреднение снижает вторую часть; если снизить $\\rho$ (разнообразные сплиты, разные признаки) — ещё лучше.

## 8.4 Гиперпараметры
- B (число моделей).\n- Сложность базового learner (глубина дерева, k для k-NN).\n- Иногда — фракция семплов/признаков (subbagging).\nСлишком малое B — высокий разброс; слишком сложный learner — рост bias/overfit каждого, но усреднение может спасать.

## 8.5 Пример
Прогноз спроса: одно дерево колеблется. Bagging 200 деревьев даёт стабильный прогноз; OOB ошибка используется для контроля.

# Question 9. Random Forest
## 9.1 Идея
Bagging деревьев + случайный выбор подмножества признаков в каждом узле (mtry) → снижает корреляцию деревьев, тем самым variance ансамбля.

## 9.2 Плюсы/минусы
**Плюсы:** высокое качество без тюнинга, встроенная OOB-оценка, устойчивость к выбросам, работает с разными типами признаков, измеряет важность признаков.\n**Минусы:** менее интерпретируем, медленнее при большом B, Gini-важность смещена в пользу многокатегориальных/числовых, может переоценивать редкие уровни, большие модели требуют памяти.\n

## 9.3 Алгоритм
1. Для b=1..B: бутстрап-сэмпл.\n2. Строим полное дерево; в каждом узле выбираем mtry признаков наугад, ищем лучший сплит только среди них.\n3. Прогноз — среднее/голос. OOB ошибка — средняя ошибка на примерах вне бутстрапа.\n

## 9.4 Гиперпараметры
- ntree (B).\n- mtry (число признаков в узле).\n- max_depth, min_samples_leaf, min_samples_split, nodesize.\n- sampfrac (если подвыборка без возвращения).\nВыбор: по OOB/CV. Малый mtry — растёт bias; большой mtry — растёт корреляция, variance. Узкие листья повышают variance, широкие — bias.

## 9.5 Пример
Классификация астмы по 50 биомаркерам. mtry ~ sqrt(50) ≈ 7. Увеличение до 20 улучшает качество: признаки сильно коррелируют, нужно рассматривать больше признаков на сплите.

# Question 10. Boosting и AdaBoost
## 10.1 Идея boosting
Последовательно обучаем слабые модели, каждая исправляет ошибки прошлой. Финальный прогноз — взвешенная сумма. Снижает bias, может повышать variance, если не контролировать шаг/число итераций.

## 10.2 Плюсы/минусы
**Плюсы:** высокая точность, работает с разными базовыми моделями, умеет важности, хорошо ловит сложные зависимости.\n**Минусы:** чувствителен к шуму и выбросам, требует аккуратного тюнинга (learning rate, итерации), может переобучаться, медленнее, чем одиночные модели.\n

## 10.3 AdaBoost (SAMME, бинарный случай, коды ±1)
Алгоритм:\n1. Инициализировать веса $w_i=1/N$.\n2. На шаге t: обучить слабый классификатор $h_t$ с учётом весов.\n3. Ошибка $err_t = \\sum w_i I\\{y_i \\neq h_t(x_i)\\} / \\sum w_i$.\n4. Вес классификатора: $\\alpha_t = \\tfrac{1}{2}\\log\\frac{1-err_t}{err_t}$.\n5. Обновить веса примеров: $w_i \\leftarrow w_i \\exp(-\\alpha_t y_i h_t(x_i))$, нормировать.\n6. Итог: $H(x) = \\operatorname{sign}\\big(\\sum_t \\alpha_t h_t(x)\\big)$.\n

## 10.4 Потери и популяционные минимизаторы
- Экспоненциальная: $L(f)=\\mathbb{E}[e^{-Y f(X)}], Y\\in\\{-1,1\\}$. Минимизатор $f^*(x)=\\tfrac{1}{2}\\log\\frac{P(Y=1|x)}{P(Y=-1|x)}$.\n- Deviance (логистическая): $L(p)=\\mathbb{E}[-Y\\log p -(1-Y)\\log(1-p)], Y\\in\\{0,1\\}$. Минимизатор $p^*(x)=P(Y=1|x)$.\nЭкспонента усиливает влияние ошибок; логистическая даёт proper scoring rule, более устойчива к шуму.\n

## 10.5 Практика
- Используйте слабые базовые learners (пни) + маленький learning rate + много итераций.\n- Следите за ранней остановкой по валидации.\n- На шумных данных — логистический вариант SAMME.R предпочтительнее.\n

# Question 11. Gradient Boosting
## 11.1 Общее описание
Бустинг, который шагает по отрицательному градиенту произвольной дифференцируемой потери в пространстве функций. Базовые модели аппроксимируют псевдоостатки.

## 11.2 Алгоритм (деревья)
1. Инициализация $F_0$: для MSE — среднее $y$, для логлосса — логит частоты.\n2. Для m=1..M:\n   - Вычислить псевдоградиенты $r_{im} = -\\partial L(y_i, F_{m-1}(x_i))/\\partial F$.\n   - Обучить дерево $h_m$ на $(x_i, r_{im})$ (обычно малая глубина).\n   - Найти шаг $\\gamma_m$ (линейный поиск: $\\arg\\min_\\gamma \\sum L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))$).\n   - Обновить $F_m = F_{m-1} + \\nu \\gamma_m h_m$.\n\nСтагнацию/переобучение контролируют через learning rate $\\nu$, M, глубину дерева, subsampling (stochastic GBM).

## 11.3 Выбор размера дерева
- Глубина 1–5: высокий bias, низкий variance, зато устойчиво.\n- Большие деревья: мощнее, но повышают variance и риск переобучения.\n- Практика: фиксировать малую глубину (или ограничивать листья) и увеличивать M; выбирать по CV/валидации.\n

## 11.4 Гиперпараметры
- M (итерации), learning rate $\\nu$, глубина/листья дерева, min_samples_leaf, min_split.\n- subsample (доля наблюдений на шаг) — для stoch. boosting снижает variance.\n- colsample (доля признаков) — снижает корреляцию деревьев.\nОшибки: большой $\\nu$ + большой M → переобучение; слишком малая глубина + малая M → недообучение.\n

## 11.5 Пример
Прогноз оттока: логистический GBM с деревьями глубины 3, $\\nu=0.05$, M=800. Подбираем M по ранней остановке на валид. AUC. Subsample=0.8 для борьбы с variance.

# Question 12. Нейросети (1 скрытый слой)
## 12.1 Что такое нейросеть
- Слой: линейная комбинация входов + нелинейная активация (ReLU/sigmoid/tanh). Один скрытый слой: $h = g(W_1 x + b_1)$, выход $\\hat y = \\phi(W_2 h + b_2)$.\n- Ключевые элементы: веса, смещения, функция активации, функция потерь, оптимизатор.\n- Универсальный аппроксиматор: с достаточным числом нейронов аппроксимирует любую непрерывную функцию на компактном множестве.\n

## 12.2 Связь с логистической регрессией
- Логистическая регрессия = нейросеть без скрытых слоёв с сигмоидой на выходе.\n- Если скрытый слой линейный (без активации), сеть остаётся линейной моделью.\n- Нелинейные активации дают полиномоподобные / кусочно-линейные аппроксимации.\n

## 12.3 Backprop (MSE) для одного скрытого слоя
1. Прямой проход: $z_1 = W_1 x + b_1$, $a_1 = g(z_1)$, $z_2 = W_2 a_1 + b_2$, $\\hat y = z_2$ (или $\\phi(z_2)$).\n2. Выходная ошибка: $\\delta_2 = (\\hat y - y) \\cdot \\phi'(z_2)$.\n3. Ошибка скрытого: $\\delta_1 = (W_2^T \\delta_2) \\cdot g'(z_1)$.\n4. Градиенты: $\\partial L/\\partial W_2 = \\delta_2 a_1^T$, $\\partial L/\\partial b_2 = \\delta_2$; $\\partial L/\\partial W_1 = \\delta_1 x^T$, $\\partial L/\\partial b_1 = \\delta_1$.\n5. Обновление: $W \\leftarrow W - \\eta \\nabla_W L$, аналогично для $b$.\n\nДля cross-entropy с сигмоидой на выходе $\\delta_2 = \\hat y - y$ (упрощается).\n

## 12.4 Проблемы и решения\n- **Взрыв/затухание градиентов.** Решения: ReLU/LeakyReLU, грамотная инициализация (He, Xavier), нормализация (BatchNorm), сквозные соединения (ResNet).\n- **Переобучение.** Dropout, L2, data augmentation, ранняя остановка по валидации.\n- **Медленное/нестабильное обучение.** Adaptive optimizers (Adam/AdamW), уменьшение learning rate по расписанию, мини-батчи, градиентный клиппинг.\n\n## 12.5 Пример\nКлассификация отзывов IMDb (мешок слов): вход 10k фич, скрытый слой 64 ReLU, выход сигмоида. Loss — BCE. Оптимизатор — Adam. Regularization — dropout 0.5 + L2. Early stopping по валид. loss.\n

# Заключительная шпаргалка (по всем билетам)
- Балансируйте bias/variance, думайте о шуме данных.\n- Потерю подбирайте под задачу и стабильность к выбросам.\n- В высоких размерностях — отбор/редукция признаков.\n- Регуляризация (Ridge/Lasso/Elastic Net) требует стандартизации и подбора $\\lambda$.\n- GLM: выберите правильную ссылку и распределение; при сепарации — штрафуйте.\n- Честная оценка — test/CV/OOB; train error — всегда оптимистичен.\n- CV: 5–10 фолдов; bootstrap — для интервальных оценок и OOB ошибок.\n- Деревья: контролируйте глубину; RF — уменьшайте mtry для декорреляции; bagging — увеличивайте B.\n- Boosting: маленький learning rate, ранняя остановка, следите за шумом.\n- GBM: мелкие деревья, много итераций, subsample.\n- Нейросети: нормализация, регуляризация, правильный optim/инициализация.\n*** End Patch json error: Expecting property name enclosed in double quotes: line 1040 column 1 (char 37313) ***!
