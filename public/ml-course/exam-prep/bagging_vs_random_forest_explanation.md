# Bagging и Random Forest: объяснение с примерами

## 1. Bagging (Bootstrap Aggregating)

### Как работает

**Шаг 1: Bootstrap-выборки**
```
Исходные данные: [x₁, x₂, x₃, x₄, x₅]

Bootstrap-выборка 1: [x₁, x₁, x₃, x₄, x₂]  ← случайная выборка с возвращением
Bootstrap-выборка 2: [x₂, x₅, x₅, x₁, x₃]
Bootstrap-выборка 3: [x₄, x₄, x₂, x₅, x₁]
...
Bootstrap-выборка B: [x₃, x₁, x₅, x₃, x₂]
```

**Шаг 2: Обучение моделей**
```
Дерево 1 обучено на выборке 1 → предсказание: ŷ₁
Дерево 2 обучено на выборке 2 → предсказание: ŷ₂
Дерево 3 обучено на выборке 3 → предсказание: ŷ₃
...
Дерево B обучено на выборке B → предсказание: ŷB
```

**Шаг 3: Агрегация**
```
Для регрессии: ŷ = (ŷ₁ + ŷ₂ + ... + ŷB) / B
Для классификации: голосование большинством
```

### Визуализация процесса

```
┌─────────────────────────────────────────────────────────┐
│                    ИСХОДНЫЕ ДАННЫЕ                      │
│  [x₁, x₂, x₃, x₄, x₅, x₆, x₇, x₈, x₉, x₁₀]              │
└─────────────────────────────────────────────────────────┘
                        │
        ┌───────────────┼───────────────┐
        │               │               │
        ▼               ▼               ▼
┌───────────┐   ┌───────────┐   ┌───────────┐
│ Bootstrap │   │ Bootstrap │   │ Bootstrap │
│ выборка 1 │   │ выборка 2 │   │ выборка B │
│           │   │           │   │           │
│ x₁ x₁ x₃  │   │ x₂ x₅ x₅  │   │ x₄ x₇ x₁₀ │
│ x₄ x₂ x₆  │   │ x₁ x₃ x₈  │   │ x₃ x₆ x₉  │
│ x₇ x₈ x₁₀ │   │ x₄ x₇ x₂  │   │ x₁ x₅ x₂  │
└─────┬─────┘   └─────┬─────┘   └─────┬─────┘
      │               │               │
      ▼               ▼               ▼
┌───────────┐   ┌───────────┐   ┌───────────┐
│  Дерево 1  │   │  Дерево 2  │   │  Дерево B  │
│           │   │           │   │           │
│   ŷ₁      │   │   ŷ₂      │   │   ŷB      │
└─────┬─────┘   └─────┬─────┘   └─────┬─────┘
      │               │               │
      └───────────────┼───────────────┘
                      ▼
            ┌─────────────────┐
            │   УСРЕДНЕНИЕ    │
            │  ŷ = среднее    │
            │  (ŷ₁, ŷ₂, ..., ŷB)│
            └─────────────────┘
```

### Пример на R

```r
# Простой bagging для регрессии
library(randomForest)

# Bagging = Random Forest с mtry = все признаки
bagging_model <- randomForest(
  y ~ ., 
  data = train_data,
  ntree = 500,           # количество деревьев
  mtry = ncol(train_data) - 1,  # ВСЕ признаки (ключевое отличие!)
  importance = TRUE
)

# Предсказание = среднее по всем деревьям
predictions <- predict(bagging_model, newdata = test_data)
```

### Почему работает

**Математика:**
- Ошибка ансамбля = Bias² + Variance
- Bagging снижает **Variance** (дисперсию)
- Не меняет **Bias** (смещение)

**Интуиция:**
- Одно дерево нестабильно → большая дисперсия
- Много деревьев усредняют ошибки → дисперсия падает
- ~63% уникальных наблюдений в каждой выборке
- ~37% наблюдений остаются "Out-of-Bag" (OOB) для валидации

---

## 2. Random Forest

### Как работает

**Random Forest = Bagging + Случайный выбор признаков**

```
┌─────────────────────────────────────────────────────────┐
│                    ИСХОДНЫЕ ДАННЫЕ                      │
│  Признаки: [age, pressure, pulse, temp, ...] (10 шт)   │
└─────────────────────────────────────────────────────────┘
                        │
        ┌───────────────┼───────────────┐
        │               │               │
        ▼               ▼               ▼
┌───────────┐   ┌───────────┐   ┌───────────┐
│ Bootstrap │   │ Bootstrap │   │ Bootstrap │
│ выборка 1 │   │ выборка 2 │   │ выборка B │
└─────┬─────┘   └─────┬─────┘   └─────┬─────┘
      │               │               │
      ▼               ▼               ▼
┌───────────────────────────────────────────────────────┐
│  При каждом разбиении узла:                           │
│                                                        │
│  Дерево 1: выбирает случайные 3 признака из 10       │
│            → [age, pressure, pulse]                   │
│                                                        │
│  Дерево 2: выбирает случайные 3 признака из 10       │
│            → [temp, age, ...]                         │
│                                                        │
│  Дерево B: выбирает случайные 3 признака из 10       │
│            → [pulse, temp, ...]                       │
└───────────────────────────────────────────────────────┘
      │               │               │
      ▼               ▼               ▼
┌───────────┐   ┌───────────┐   ┌───────────┐
│  Дерево 1  │   │  Дерево 2  │   │  Дерево B  │
│ (разные    │   │ (разные    │   │ (разные    │
│  признаки) │   │  признаки) │   │  признаки) │
└─────┬─────┘   └─────┬─────┘   └─────┬─────┘
      │               │               │
      └───────────────┼───────────────┘
                      ▼
            ┌─────────────────┐
            │   УСРЕДНЕНИЕ    │
            └─────────────────┘
```

### Пример построения одного дерева

```
Исходные данные: 1000 наблюдений, 10 признаков
mtry = 3 (выбираем 3 случайных признака на разбиение)

Узел 1 (корень):
  └─ Случайно выбраны признаки: [age, pressure, pulse]
  └─ Лучшее разбиение: age < 65
     ├─ Левая ветвь: 600 наблюдений
     └─ Правая ветвь: 400 наблюдений

Узел 2 (левая ветвь):
  └─ Случайно выбраны признаки: [temp, pulse, ...]  ← НОВЫЙ набор!
  └─ Лучшее разбиение: temp > 37.5
     ├─ Левая ветвь: 350 наблюдений
     └─ Правая ветвь: 250 наблюдений

Узел 3 (правая ветвь от корня):
  └─ Случайно выбраны признаки: [age, temp, ...]  ← ЕЩЁ один набор!
  └─ Лучшее разбиение: ...
```

### Пример на R

```r
# Random Forest
rf_model <- randomForest(
  y ~ ., 
  data = train_data,
  ntree = 500,                    # количество деревьев
  mtry = sqrt(ncol(train_data)-1), # √p признаков (ключевое отличие!)
  importance = TRUE
)

# Предсказание
predictions <- predict(rf_model, newdata = test_data)
```

---

## 3. Различия: Bagging vs Random Forest

### Таблица сравнения

| Параметр | Bagging | Random Forest |
|----------|---------|---------------|
| **Выбор признаков** | Все признаки на каждом разбиении | Случайное подмножество (`mtry`) |
| **Разнообразие деревьев** | Только за счёт данных | За счёт данных + признаков |
| **Корреляция деревьев** | Выше | Ниже |
| **Снижение дисперсии** | Умеренное | Сильнее |
| **Скорость обучения** | Быстрее | Чуть медленнее |
| **Переобучение** | Возможно при сильных признаках | Меньше риск |

### Визуальное сравнение

**Bagging:**
```
Дерево 1: [все признаки] → использует age, pressure, pulse
Дерево 2: [все признаки] → использует age, pressure, pulse  ← похоже!
Дерево 3: [все признаки] → использует age, pressure, pulse  ← похоже!
```
→ Деревья коррелированы, если один признак очень сильный

**Random Forest (mtry=3):**
```
Дерево 1: [age, pressure, pulse] → использует age
Дерево 2: [temp, age, ...]      → использует temp      ← разное!
Дерево 3: [pulse, temp, ...]     → использует pulse     ← разное!
```
→ Деревья менее коррелированы, больше разнообразия

---

## 4. Почему Random Forest лучше

### Математическая формула

**Дисперсия ансамбля:**
```
Var(ŷ_ensemble) = σ²/B + (B-1)/B × ρ × σ²
```

Где:
- `σ²` = дисперсия одного дерева
- `ρ` = корреляция между деревьями
- `B` = количество деревьев

**Вывод:** Чтобы снизить дисперсию, нужно снизить `ρ` (корреляцию)!

**Как Random Forest снижает корреляцию:**
1. ✅ Разные bootstrap-выборки (как в bagging)
2. ✅ **Разные наборы признаков** (уникально для RF!)

### Пример: почему это важно

**Сценарий:** У вас 100 признаков, но только 2 важных (age, pressure)

**Bagging (mtry = 100):**
- Каждое дерево видит все признаки
- Почти все деревья выберут age или pressure на первом разбиении
- Деревья очень похожи → высокая корреляция → плохое усреднение

**Random Forest (mtry = 10):**
- На каждом разбиении выбирается только 10 из 100 признаков
- Вероятность, что оба важных признака попадут = (10/100)² = 1%
- Деревья вынуждены использовать разные признаки
- Деревья разнообразнее → низкая корреляция → лучшее усреднение

---

## 5. Практический пример: ICU данные

```r
library(randomForest)

# Данные о пациентах в реанимации
data(icu)  # 200 наблюдений, 10 признаков

# 1. Одно дерево (базовая модель)
tree1 <- randomForest(
  sta ~ ., 
  data = icu, 
  ntree = 1,                    # одно дерево
  mtry = ncol(icu) - 1          # все признаки
)
# Ошибка: ~30%

# 2. Bagging (500 деревьев, все признаки)
bagging <- randomForest(
  sta ~ ., 
  data = icu,
  ntree = 500,
  mtry = ncol(icu) - 1          # ВСЕ признаки
)
# Ошибка: ~22%

# 3. Random Forest (500 деревьев, √p признаков)
rf <- randomForest(
  sta ~ ., 
  data = icu,
  ntree = 500,
  mtry = sqrt(ncol(icu) - 1)    # √10 ≈ 3 признака
)
# Ошибка: ~18% ← ЛУЧШЕ!
```

### График OOB ошибки

```
Ошибка
  │
30%│     ●
   │    ╱
25%│   ╱  ●
   │  ╱    ╲
20%│ ╱      ●───●───●───●  ← Random Forest
   │╱              ╲
15%│                ●───●───●  ← Bagging
   │
   └──────────────────────────→ Количество деревьев
     100  200  300  400  500
```

---

## 6. Ключевые параметры

### `ntree` (количество деревьев)
- **Больше** → меньше дисперсия, но дольше обучение
- **Оптимум:** обычно 500-1000 (после этого выгода минимальна)

### `mtry` (признаков на разбиение)
- **Bagging:** `mtry = p` (все признаки)
- **Random Forest:** 
  - Классификация: `mtry = √p`
  - Регрессия: `mtry = p/3`
- **Меньше mtry** → больше разнообразия, но может пропустить важные признаки

### Визуализация влияния mtry

```
mtry = p (все признаки)
  └─ Деревья похожи → высокая корреляция
  └─ Быстрое обучение
  └─ Риск переобучения

mtry = √p (стандарт)
  └─ Баланс разнообразия и качества
  └─ Оптимально для большинства задач

mtry = 1 (очень мало)
  └─ Максимальное разнообразие
  └─ Медленное обучение
  └─ Может пропустить важные признаки
```

---

## 7. Когда что использовать

### Используйте Bagging, если:
- ✅ Хотите простой ансамбль
- ✅ Базовая модель уже достаточно разнообразна
- ✅ Используете не деревья (нейросети, SVM)

### Используйте Random Forest, если:
- ✅ Работаете с деревьями решений
- ✅ Много признаков (снижает переобучение)
- ✅ Нужна важность признаков (встроенная)
- ✅ Нужна OOB оценка ошибки без валидации

---

## 8. Резюме

**Bagging:**
```
Bootstrap → Обучение на всех признаках → Усреднение
```

**Random Forest:**
```
Bootstrap → Обучение на СЛУЧАЙНЫХ признаках → Усреднение
           ↑
      Ключевое отличие!
```

**Главная идея:** Random Forest добавляет дополнительную рандомизацию (выбор признаков), что снижает корреляцию между деревьями и улучшает обобщение.

